{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from models.modules.encoder import RnnUserEncoder\n",
    "from models.modules.weighter import CnnWeighter\n",
    "from utils.util import save_pickle, load_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/data/v-pezhang/Data/MIND/CosMos/news.tsv\") as f:\n",
    "    g = open(\"/data/v-pezhang/Data/MIND/CosMos/news-1.tsv\", \"w\")\n",
    "    for i,line in enumerate(f):\n",
    "        if i > 10000:\n",
    "            break\n",
    "        g.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"/data/v-pezhang/Data/MIND/CosMos/news.tsv\", header=None, index_col=None, quoting=3, usecols=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle(data[data[1] == \"news\"].index.to_numpy() + 1, \"data/cache/MIND/CosMos/sports_idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAA7oHF</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>Americans agree on one part of the U.S.-Russia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAAKxEy</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>Controversial Police Encounters Fast Facts</td>\n",
       "      <td>Here's a look at controversial police encounte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AAAYVXp</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Corrections &amp; Clarifications</td>\n",
       "      <td>To report Corrections &amp; Clarifications, contac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AAAHYf1</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>Louis Farrakhan Fast Facts</td>\n",
       "      <td>Here's a look at the life of Louis Farrakhan ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AAAZ2H6</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>Roe v. Wade Fast Facts</td>\n",
       "      <td>Here's a look at the US Supreme Court case Roe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>AAKBWqu</td>\n",
       "      <td>news</td>\n",
       "      <td>newspolitics</td>\n",
       "      <td>New footage shows a boat parade of Trump suppo...</td>\n",
       "      <td>On Memorial Day, Trump supporters celebrated b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>AAKBZY4</td>\n",
       "      <td>news</td>\n",
       "      <td>newspolitics</td>\n",
       "      <td>Graham says Israel will request $1 billion fro...</td>\n",
       "      <td>Sen. Lindsey Graham (R-S.C.) said Tuesday that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>AAKBWwh</td>\n",
       "      <td>news</td>\n",
       "      <td>newscrime</td>\n",
       "      <td>FDLE agent injured in shooting during Kissimme...</td>\n",
       "      <td>An undercover agent with the Florida Departmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>AAKBYA6</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>Chauvin makes first court appearance on civil ...</td>\n",
       "      <td>Derek Chauvin, the fired Minneapolis police of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>AAKBYg3</td>\n",
       "      <td>news</td>\n",
       "      <td>newsus</td>\n",
       "      <td>Oshkosh school board confirms Bryan Davis as d...</td>\n",
       "      <td>OSHKOSH - The Oshkosh Area School District boa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3846 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0     1                         2  \\\n",
       "2     AAA7oHF  news                 newsworld   \n",
       "6     AAAKxEy  news                    newsus   \n",
       "15    AAAYVXp  news  newsscienceandtechnology   \n",
       "22    AAAHYf1  news                 newsworld   \n",
       "32    AAAZ2H6  news                    newsus   \n",
       "...       ...   ...                       ...   \n",
       "9989  AAKBWqu  news              newspolitics   \n",
       "9990  AAKBZY4  news              newspolitics   \n",
       "9991  AAKBWwh  news                 newscrime   \n",
       "9992  AAKBYA6  news                    newsus   \n",
       "9993  AAKBYg3  news                    newsus   \n",
       "\n",
       "                                                      3  \\\n",
       "2     Americans agree on one part of the U.S.-Russia...   \n",
       "6            Controversial Police Encounters Fast Facts   \n",
       "15                         Corrections & Clarifications   \n",
       "22                           Louis Farrakhan Fast Facts   \n",
       "32                               Roe v. Wade Fast Facts   \n",
       "...                                                 ...   \n",
       "9989  New footage shows a boat parade of Trump suppo...   \n",
       "9990  Graham says Israel will request $1 billion fro...   \n",
       "9991  FDLE agent injured in shooting during Kissimme...   \n",
       "9992  Chauvin makes first court appearance on civil ...   \n",
       "9993  Oshkosh school board confirms Bryan Davis as d...   \n",
       "\n",
       "                                                      4  \n",
       "2                                                   NaN  \n",
       "6     Here's a look at controversial police encounte...  \n",
       "15    To report Corrections & Clarifications, contac...  \n",
       "22    Here's a look at the life of Louis Farrakhan ,...  \n",
       "32    Here's a look at the US Supreme Court case Roe...  \n",
       "...                                                 ...  \n",
       "9989  On Memorial Day, Trump supporters celebrated b...  \n",
       "9990  Sen. Lindsey Graham (R-S.C.) said Tuesday that...  \n",
       "9991  An undercover agent with the Florida Departmen...  \n",
       "9992  Derek Chauvin, the fired Minneapolis police of...  \n",
       "9993  OSHKOSH - The Oshkosh Area School District boa...  \n",
       "\n",
       "[3846 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[1] == \"news\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnWeighter(nn.Module):\n",
    "    def __init__(self, manager):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=manager.gate_embedding_dim,\n",
    "                out_channels=manager.gate_hidden_dim,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.weightPooler = nn.Sequential(\n",
    "            nn.Linear(manager.gate_hidden_dim, manager.gate_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(manager.dropout_p),\n",
    "            nn.Linear(manager.gate_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "        nn.init.xavier_normal_(self.cnn[0].weight)\n",
    "\n",
    "\n",
    "    def _compute_weight(self, embeddings):\n",
    "        weights = self.weightPooler(embeddings).squeeze(-1)\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_id: [B, L]\n",
    "            attn_mask: [B, L]\n",
    "\n",
    "        Returns:\n",
    "            weights: [B, L]\n",
    "        \"\"\"\n",
    "        original_shape = embedding.shape[:-1]\n",
    "        cnn_input = embedding.transpose(-1, -2)\n",
    "        conv_embedding = self.cnn(cnn_input).transpose(-1, -2).view(*original_shape, -1)\n",
    "        weight = self._compute_weight(conv_embedding)\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.CnnWeighter'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151093248.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class m:\n",
    "    hidden_dim = 768\n",
    "    gate_embedding_dim = 768\n",
    "    gate_hidden_dim = 768\n",
    "    dropout_p = 0.1\n",
    "    vocab_size = 30522\n",
    "manager = m()\n",
    "model = CnnWeighter(manager)\n",
    "x = torch.rand(1, 32, manager.gate_embedding_dim)\n",
    "\n",
    "# FLOPs of original BERT\n",
    "macs, params = profile(model, inputs=(x,))\n",
    "macs * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.743148544"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLOPs of BaseBert\n",
    "L = 32\n",
    "\n",
    "embedding = 2 * L * 768 * 3\n",
    "bert_project = 12 * L * 64 * 64* 2 * 3\n",
    "bert_attn = 12 * L * 64 * L * 2 + 12 * L * L * 64 * 2\n",
    "bert_intm =  L * 768 * 768 * 2 +  L * 768 * 2 +  L * 768 * 3072 * 4\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12\n",
    "\n",
    "all = embedding + bert\n",
    "all * 12 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4162535424"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLOPs of Synthesizer\n",
    "L = 32\n",
    "\n",
    "embedding = 2 * L * 768 * 3\n",
    "bert_project = 12 * L * 64 * 64 * 2\n",
    "synthesizer_attn = 12 * (L * 64 * L * 2 + L * L * L * 2) + 12 * L * L * 64 * 2\n",
    "bert_intm =  L * 768 * 768 * 2 +  L * 768 * 2 +  L * 768 * 3072 * 4\n",
    "\n",
    "bert = (bert_project + synthesizer_attn + bert_intm) * 12\n",
    "\n",
    "all = embedding + bert\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.60085248"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLOPs of baseline pruners\n",
    "K = 8\n",
    "\n",
    "embedding = 2 * K * 768 * 3\n",
    "bert_project = 12 * K * 64 * 2 * 3 * 64\n",
    "bert_attn = 12 * K * 64 * K * 2 + 12 * K * K * 64 * 2\n",
    "bert_intm =  K * 768 * 768 * 2 +  K * 768 * 2 +  K * 768 * 3072 * 4\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12\n",
    "\n",
    "pruner = embedding + bert\n",
    "pruner * 12 / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.7047648, 8659360.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# FLOPs of InfoGate\n",
    "L = 32\n",
    "K = 8\n",
    "D = 300\n",
    "\n",
    "gate_embedding = 2 * L * D\n",
    "# gate_weighting = 151093248\n",
    "gate_weighting = L * 300 * 300 * 3\n",
    "\n",
    "gate_sort = L * math.log2(L)\n",
    "gate_all = gate_embedding + gate_weighting + gate_sort\n",
    "\n",
    "embedding = 2 * K * 768 * 3\n",
    "bert_project = 12 * K * 64 * 2 * 3 * 64\n",
    "bert_attn = 12 * K * 64 * K * 2 + 12 * K * K * 64 * 2\n",
    "bert_intm =  K * 768 * 768 * 2 +  K * 768 * 2 +  K * 768 * 3072 * 4\n",
    "\n",
    "bert = embedding + (bert_project + bert_attn + bert_intm) * 12\n",
    "\n",
    "infogate = bert + gate_all\n",
    "infogate * 12 / 1e9, gate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.829533056, 352390048.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FLOPs of InfoGate(Trans)\n",
    "L = 32\n",
    "K = 8\n",
    "D = 300\n",
    "\n",
    "gate_embedding = 2 * L * D\n",
    "gate_weighting = 12 * L * 64 * 64 * 2 * 3 + 12 * L * 64 * L * 2 + 12 * L * L * 64 * 2 + L * 768 * 768 * 2 +  L * 768 * 2 +  L * 768 * 3072 * 4\n",
    "gate_sort = L * math.log2(L)\n",
    "gate_all = gate_embedding + gate_weighting + gate_sort\n",
    "\n",
    "embedding = 2 * K * 768 * 3\n",
    "bert_project = 12 * K * 64 * 2 * 3 * 64\n",
    "bert_attn = 12 * K * 64 * K * 2 + 12 * K * K * 64 * 2\n",
    "bert_intm =  K * 768 * 768 * 2 +  K * 768 * 2 +  K * 768 * 3072 * 4\n",
    "\n",
    "bert = (bert_project + bert_attn + bert_intm) * 12\n",
    "\n",
    "infogate = embedding + bert + gate_all\n",
    "infogate * 12 / 1e9, gate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37.973313408, 2114371744.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "# FLOPs of KeyBERT\n",
    "L = 32\n",
    "K = 8\n",
    "D = 300\n",
    "\n",
    "embedding = 2 * L * 768 * 3\n",
    "bert_project = 12 * L * 64 * 64* 2 * 3\n",
    "bert_attn = 12 * L * 64 * L * 2 + 12 * L * L * 64 * 2\n",
    "bert_intm =  L * 768 * 768 * 2 +  L * 768 * 2 +  L * 768 * 3072 * 4\n",
    "gate_sort = L * math.log2(L)\n",
    "gate_all = embedding + (bert_project + bert_attn + bert_intm) * 6 + gate_sort\n",
    "\n",
    "embedding = 2 * K * 768 * 3\n",
    "bert_project = 12 * K * 64 * 2 * 3 * 64\n",
    "bert_attn = 12 * K * 64 * K * 2 + 12 * K * K * 64 * 2\n",
    "bert_intm =  K * 768 * 768 * 2 +  K * 768 * 2 +  K * 768 * 3072 * 4\n",
    "\n",
    "bert = embedding + (bert_project + bert_attn + bert_intm) * 12\n",
    "\n",
    "infogate = bert + gate_all\n",
    "infogate * 12 / 1e9, gate_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7716033974308345, 6.222750033481267, 8.064700148030473)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruner / infogate, all / infogate, all / pruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.sparse.Embedding'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class '__main__.M'>. Treat it as zero Macs and zero Params.\u001b[00m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5761/2115364143.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/v-pezhang/Envs/nn/lib/python3.9/site-packages/thop/profile.py\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(model, inputs, custom_ops, verbose)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdfs_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/v-pezhang/Envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5761/2115364143.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30522\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/v-pezhang/Envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/v-pezhang/Envs/nn/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/data/v-pezhang/Envs/nn/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(30522, 768)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "m = M()\n",
    "x = torch.rand(1, 32, 768)\n",
    "macs, params = profile(m, inputs=(x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
